{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "We will now train our first classifier.  We will use the titanic dataset for this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data using pandas as we learnt in the previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# we are loading data from github. \n",
    "dataurl = 'https://github.com/rrr-uom-projects/MPiCRT-AI/raw/main/Data/titanic.csv' \n",
    "pax = pd.read_csv(dataurl, sep = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to understand the data we have to start making sense of it. Here is a short description of the series:\n",
    "\n",
    "- **PassengerId** Arbitrary nr between 1 and 841\n",
    "- **Survived** Weather Survived or not: 0 = No, 1 = Yes\n",
    "- **Pclass** Ticket class: 1 = 1st, 2 = 2nd, 3 = 3rd\n",
    "- **Name** Name of the Passenger\n",
    "- **Sex** Female/male\n",
    "- **Age** Age in years\n",
    "- **SibSp** No. of siblings / spouses aboard the Titanic\n",
    "- **Parch** No. of parents / children aboard the Titanic\n",
    "- **Ticket** Ticket number\n",
    "- **Fare** Passenger fare\n",
    "- **Cabin** Cabin number\n",
    "- **Embarked** Port of Embarkation:C = Cherbourg, Q = Queenstown, S = Southampton\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sort the categorical variables correctly here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pax['Sex'] = pax['Sex'].astype('category')\n",
    "pax['Survived'] = pax['Survived'].astype(\"category\")\n",
    "pax['Pclass'] = pax['Pclass'].astype(\"category\")\n",
    "pax['Embarked'] = pax['Embarked'].astype(\"category\")\n",
    "\n",
    "pax.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "During the last tutorial we explored the data and extracted some extra bits from some variables.  Let's bring the relevant code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing Age\n",
    "\n",
    "We learnt how to impute age accounting for Sex, Pclass, Embarked, etc. Let's copy the relevant code here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medianAges = pax.groupby(['Sex','Pclass','Embarked'], observed=True)[['Age']].median()\n",
    "medianAges = medianAges.reset_index()\n",
    "\n",
    "def getMedianAgeForCategory(row):\n",
    "    # using the dataframe medianAges created above.\n",
    "    condition = (\n",
    "        (medianAges['Sex'] == row['Sex']) & \n",
    "        (medianAges['Pclass'] == row['Pclass']) & \n",
    "        (medianAges['Embarked'] == row['Embarked'])\n",
    "    ) \n",
    "    return medianAges[condition]['Age'].values[0]\n",
    "\n",
    "def imputeIfNeeded(row):\n",
    "    return getMedianAgeForCategory(row) if np.isnan(row['Age']) else row['Age']\n",
    "\n",
    "#let's make a copy of the values before imputing\n",
    "pax['Age'] = pax.apply(imputeIfNeeded, axis=1)\n",
    "pax.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titles and tytle types\n",
    "We also extracted titles from the passanger's name, and coded this title based on domain knowledge. \n",
    "\n",
    "Let's copy the relevant code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to cast the type of the Name series to str. \n",
    "pax['Name'] = pax['Name'].astype('string')\n",
    "surnamefirstnames = pax['Name'].str.split(',')  # this splits the string by the token given (,)\n",
    "pax['Surname'] = surnamefirstnames.str.get(0)   # here we get the first bit of the divided sentence\n",
    "afterComma = surnamefirstnames.str.get(1).str.split('.')# this splits the string by the token given (.)\n",
    "pax['Title'] = afterComma.str.get(0).str.strip()        # here we get the first bit of the divided sentence and eliminate empty spaces\n",
    "\n",
    "Title_Dictionary = {\n",
    "    \"Capt\": \"Officer\",\n",
    "    \"Col\": \"Officer\",\n",
    "    \"Major\": \"Officer\",\n",
    "    \"Jonkheer\": \"Royalty\",\n",
    "    \"Don\": \"Royalty\",\n",
    "    \"Sir\" : \"Royalty\",\n",
    "    \"Dr\": \"Officer\",\n",
    "    \"Rev\": \"Officer\",\n",
    "    \"the Countess\":\"Royalty\",\n",
    "    \"Mme\": \"Mrs\",\n",
    "    \"Mlle\": \"Miss\",\n",
    "    \"Ms\": \"Mrs\",\n",
    "    \"Mr\" : \"Mr\",\n",
    "    \"Mrs\" : \"Mrs\",\n",
    "    \"Miss\" : \"Miss\",\n",
    "    \"Master\" : \"Master\",\n",
    "    \"Lady\" : \"Royalty\"\n",
    "}\n",
    "pax['TitleType'] = pax['Title'].map(Title_Dictionary)\n",
    "pax['TitleType'] = pax['TitleType'].astype('category')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Family sizes and types\n",
    "And we also created a new variable quantifying the size of the family for each passanger, and classified it in three classes: single, small family, large family. Let's bring the relevant code here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pax['FamilySize'] = pax['SibSp']+pax['Parch']+1 \n",
    "def getFamilyType(famsize):\n",
    "    return 'single' if famsize == 1 else ('smallFamily' if famsize < 5 else 'largeFamily')\n",
    "\n",
    "pax['FamilyType'] = pax['FamilySize'].apply(getFamilyType)\n",
    "pax['FamilyType'] = pax['FamilyType'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminate variables \n",
    "\n",
    "Now we have extracted extra information from the data stored for each passanger. We can now clean up our dataframe in preparation to model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pax.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pax.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanpax = pax.loc[:,['Survived', 'Pclass', 'Sex', 'Age', 'SibSp',\n",
    "       'Parch', 'Fare', 'Embarked', 'TitleType', 'FamilySize', 'FamilyType']]\n",
    "cleanpax.dropna(inplace=True)\n",
    "cleanpax.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data splitting\n",
    "\n",
    "Before we do any training, let's divide the dataset in *training* and *validation*.  Ideally, we will have another dataset, *test*, to test for generalisability.  Kaggle kept a good portion of the data as test.  We won't use it in our tutorial.  But if you feel like seeing how generalisable are your models, join Kaggle and submit your solutions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = cleanpax.loc[:,'Survived'] # This is the target!\n",
    "X = cleanpax.loc[:, cleanpax.columns != 'Survived'] # This are the features/variables we wll use to predict\n",
    "\n",
    "# to divide the data in train/validation, we an use train_test_split from sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, x_val, Y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=1234) # train 80%, validation 20%\n",
    "\n",
    "print(f'Features for train/validation datasets: {X_train.shape} and  {x_val.shape}' )\n",
    "print(f'  In percentages: {100*X_train.shape[0]/pax.shape[0]:.2f}% and  {100*x_val.shape[0]/pax.shape[0]:.2f}%' )\n",
    "survcounts = [Y_train.value_counts(),y_val.value_counts()]\n",
    "print(f'Percentage survived for train/validation datasets: {100*survcounts[0][1]/Y_train.shape[0]:.2f}, {100*survcounts[1][1]/y_val.shape[0]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use X_train and Y_train to create our models, and use x_test and y_test to test for overfitting!  We will learn more about this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Data pre-processing\n",
    "We will use SVM and RFC for our classifiers, as implemented in sklearn.  These implementation require all features converted to *numerical* features. So we need to convert the categorical values into numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary categories\n",
    "First, let's do the binary categories: only two possible values are allowed.  In this case, one of the values can be mapped to 0 and the other one to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num = X_train\n",
    "X_train_num['Sex'] = X_train_num['Sex'].map({'male': 0, 'female': 1}).astype(int)\n",
    "\n",
    "Y_train_num = Y_train.astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categories with multiple values\n",
    "Categorical variables with multiple values are a bit more tricky. In this case, we can use the function get_dummies to convert them to a set of columns, one column per category value.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num = pd.get_dummies(X_train_num, prefix='FamilyType',columns=['FamilyType'],dtype=int)\n",
    "X_train_num.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num = pd.get_dummies(X_train_num, prefix='Embarked',columns=['Embarked'],dtype=int)\n",
    "X_train_num = pd.get_dummies(X_train_num, prefix='TitleType',columns=['TitleType'],dtype=int)\n",
    "X_train_num = pd.get_dummies(X_train_num, prefix='Pclass',columns=['Pclass'],dtype=int)\n",
    "\n",
    "X_train_num.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to repeat the same operations to the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_num = x_val\n",
    "x_val_num['Sex'] = x_val_num['Sex'].map({'male': 0, 'female': 1}).astype(int)\n",
    "x_val_num = pd.get_dummies(x_val_num, prefix='FamilyType',columns=['FamilyType'],dtype=int)\n",
    "x_val_num = pd.get_dummies(x_val_num, prefix='Embarked',columns=['Embarked'],dtype=int)\n",
    "x_val_num = pd.get_dummies(x_val_num, prefix='TitleType',columns=['TitleType'],dtype=int)\n",
    "x_val_num = pd.get_dummies(x_val_num, prefix='Pclass',columns=['Pclass'],dtype=int)\n",
    "print(x_val_num.info())\n",
    "\n",
    "y_val_num = y_val.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "Let's classify first with SVMs.  That means finding the best weights, bias and support vectors in the training dataset.  This is done easily using sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "classifier = svm.SVC(kernel=\"rbf\", gamma=0.5, probability=True)\n",
    "classifier.fit(X_train_num, Y_train_num)  # here is where the magic happens ;-)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the fit\n",
    "\n",
    "Now let's see how the model works for the data we kept apart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = classifier.score(x_val_num, y_val_num)\n",
    "print('Accuracy: ',score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not that good, eh?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sometimes less is more!\n",
    "Let's try with less variables.  We learnt that Sex, Age and Pclass were very strongly correlated to Survived in the previous tutorial... Let's see if this works better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_small = X_train_num.loc[:,['Sex', 'Age', 'Pclass_1', 'Pclass_2', 'Pclass_3']]\n",
    "x_val_small = x_val_num.loc[:,['Sex', 'Age', 'Pclass_1', 'Pclass_2', 'Pclass_3']]\n",
    "classifier.fit(X_train_small, Y_train_num)  # here is where the magic happens ;-)\n",
    "score = classifier.score(x_val_small, y_val_num)\n",
    "\n",
    "print('Accuracy: ',score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that more data/more features is not always better!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifiers\n",
    "\n",
    "Let's explore now random forest classifiers.  \n",
    "Let's choose 100 trees in the forest and to keep out-of-the-bag score for an idea of how well the training went.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, oob_score=True)\n",
    "rf.fit(X_train_num, Y_train_num.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "y_pred = rf.predict(x_val_num)\n",
    "\n",
    "print( f'Accuracy: {metrics.accuracy_score(y_val, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about having less features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_small = RandomForestClassifier(n_estimators=100, oob_score=True)\n",
    "rf_small.fit(X_train_small, Y_train_num.astype(int))\n",
    "y_pred = rf_small.predict(x_val_small)\n",
    "\n",
    "print( f'Accuracy: {metrics.accuracy_score(y_val, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the performance was not better with less figures. Trees are able to 'squeeze' more information from the other dimensions!  At the same time, you need to be more aware of overfitting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importances\n",
    "Another great feature of RFC is that you can investigate which features were used more frequently, which you could use to make the trees simpler.  Let's see this for the first forest we trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pd.Series(rf.feature_importances_, index=X_train_num.columns).sort_values(ascending=True)\n",
    "feature_imp.plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other classification metrics\n",
    "We can use classification_report() to get a summary of most common classification metrics.  Let's see these metrics for the RFCs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(x_val_num)\n",
    "print(metrics.classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find information on these metrics in the scikit-learn documentation: https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics \n",
    "\n",
    "We are done for today!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
